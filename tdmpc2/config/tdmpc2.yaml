defaults:
    - override hydra/launcher: submitit_local

# environment
task: stack-cube
obs: state
num_envs: 16

# evaluation
checkpoint: ???
eval_episodes: 10
eval_freq: 50000

# training
algorithm: ???
steps: 10_000_000
batch_size: 256
steps_per_update: 4 # utd = steps_per_update / num_envs (in this implementation)
reward_coef: 0.1
value_coef: 0.1
consistency_coef: 20
rho: 0.5
lr: 3e-4
enc_lr_scale: 0.3
grad_clip_norm: 20
tau: 0.01
discount_denom: 5
discount_min: 0.95
discount_max: 0.995
discount_hardcoded: 0.0 # Overrides heuristic discount calculation
buffer_size: 1_000_000
exp_name: default
data_dir: ???

# planning
mpc: true
iterations: 6
num_samples: 512
num_elites: 64
num_pi_trajs: 24
horizon: 3
min_std: 0.05
max_std: 2
temperature: 0.5

# actor
log_std_min: -10
log_std_max: 2
entropy_coef: 1e-4

# critic
num_bins: 101
vmin: -10
vmax: +10

# architecture
model_size: ???
num_enc_layers: 2
enc_dim: 256
num_channels: 32
mlp_dim: 512
latent_dim: 512
task_dim: 96
num_q: 5
dropout: 0.01
simnorm_dim: 8

# logging
wandb_project: tdmpc2-drS
wandb_entity: alopez
wandb_silent: true
disable_wandb: true
save_csv: false
save_freq: 1_000_000

# drS
drS_ckpt: ???

# misc
save_video: true
save_agent: true
render_obs: false
seed: 1

# convenience
work_dir: ???
task_title: ???
multitask: ???
tasks: ???
obs_shape: ???
action_dim: ???
episode_length: ???
obs_shapes: ???
action_dims: ???
episode_lengths: ???
seed_steps: ???
bin_size: ???

# environments
maniskill:
    action_repeat: 2
    obs_keys: ["agent", "image"]
    max_episode_steps: 100
    camera:
        image_size: 128 # 64, 128, 256

metaworld:
    action_repeat: 2
    max_episode_steps: 100
    render_mode: "rgb_array" # "human", "rgb_array"
    reward_mode: ???
    obs: ???
    camera:
        image_size: 224 # 64, 128, 256

bigym:
    action_repeat: 1
    render_mode: "rgb_array" # "human", "rgb_array"
    obs: ???
    camera:
        image_size: 224 # 64, 128, 256
